from __future__ import annotations

import subprocess
import sys
import time
from typing import Optional

import my_assignment
import pytest
from dotenv import load_dotenv


def test_empty():
    """Calling add_all on an empty list should return 0."""
    assert my_assignment.add_all([]) == 0


def test_single_element():
    """Calling add_all on a list with one number should return that number."""
    assert my_assignment.add_all([10]) == 10


def test_two_elements(request):
    """Calling add_all on a list with two numbers should return the sum of those numbers."""
    try:
        test_input = [2, 3]
        expected = 5
        assert my_assignment.add_all(test_input) == expected
    except AssertionError as e:
        start = time.perf_counter()
        llm_feedback = run_llm(
            submission="./my_assignment.py",
            solution="./assignment_solution.py",
            prompt_text=f"Please BRIEFLY explain in up to five English sentences the following error to a beginner Python programmer. Assume they implemented the add_all function, and the instructor provided the assertion. The expected return value was {expected}. Do not write any code.\n\n```{e}\n```",
            scope="code",
            model="remote",
            remote_model="deepseek-v2.5:236b-q5_1",
            # remote_model="DeepSeek-V3-0324-UD-Q2_K_XL",
            # llama_mode="server",
        )
        end = time.perf_counter()
        elapsed = end - start

        request.node.add_marker(
            pytest.mark.markus_message(
                f'ðŸ¤– *The following was generated by an AI:* {llm_feedback}\n\nLLM response time: {elapsed:.6f} seconds'
            )
        )

        raise


def test_many_elements():
    """Calling add_all on a list with many numbers should return the sum of all of the numbers."""
    numbers = list(range(10))
    assert my_assignment.add_all(numbers) == 45


def test_all_zeros_list():
    """Calling add_all on a list with all zeros should return 0."""
    assert my_assignment.add_all([0, 0, 0, 0]) == 0


def run_llm(
    submission: str,
    model: str,
    scope: str,
    output: Optional[str] = None,
    solution: Optional[str] = None,
    prompt_custom: Optional[str] = None,
    question: Optional[str] = None,
    prompt_text: Optional[str] = None,
    prompt: Optional[str] = None,
    submission_image: Optional[str] = None,
    remote_model: Optional[str] = None,
    llama_mode: Optional[str] = None,
) -> str:
    """
    Executes the LLM feedback generator script and captures its output.

    Args:
        submission: The submission type.
        model: The LLM model to use.
        scope: The feedback generation scope ('code', 'image', or 'text').
        output: The output format type.
        prompt_custom: A path to a custom prompt file.
        question: Optional assignment question text.
        prompt_text: Custom string input prompt for the LLM.
        prompt: Name of predefined prompt file to use.
        submission_image: Optional path to a submission image file.
        remote_model: Optional name of a remote model to use. Implies model='remote'

    Returns:
        The output from the LLM feedback generator as a string, or an error message.
    """
    load_dotenv()
    llm_command = [sys.executable, "-m", "ai_feedback", "--scope", scope, "--submission", submission, "--model", model]
    if solution is not None:
        llm_command += ["--solution", solution]
    if question is not None:
        llm_command += ["--question", question]
    if prompt_custom is not None:
        llm_command += ["--prompt_custom", prompt_custom]
    if prompt is not None:
        llm_command += ["--prompt", prompt]
    if prompt_text is not None:
        llm_command += ["--prompt_text", prompt_text]
    if output is not None:
        llm_command += ["--output", output]
    if submission_image is not None:
        llm_command += ["--submission_image", submission_image]
    if remote_model is not None:
        llm_command += ["--remote_model", remote_model]
    if llama_mode is not None:
        llm_command += ["--llama_mode", llama_mode]

    try:
        llm_result = subprocess.run(llm_command, capture_output=True, text=True, check=True)
    except subprocess.CalledProcessError as e:
        return f"Error calling LLM API:\n{e.stderr.strip()}"
    else:
        return llm_result.stdout.strip()
